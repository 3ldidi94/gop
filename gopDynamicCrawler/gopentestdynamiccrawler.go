package gopdynamiccrawler

import (
	"fmt"
	"time"

	gopstaticcrawler "github.com/hophouse/gop/gopStaticCrawler"
	"github.com/hophouse/gop/utils"
)

func RunCrawlerCmd() {
	begin := time.Now()

	// bars
	progressBars := utils.InitWaitGroupBar()

	// Init the crawler
	InitCrawler()

	utils.CrawlerBar = progressBars.AddBar("Crawler", true)

	if *GoCrawlerOptions.ScreenshotPtr == true {
		//utils.ScreenshotBar = progressBars.AddBar("Screenshot", false)
	}

	fmt.Printf("\n")
	fmt.Printf("[+] Crawling from URL: %s\n\n", gopstaticcrawler.Yellow(*GoCrawlerOptions.UrlPtr))

	// Launch the workers
	for i := 0; i < *GoCrawlerOptions.ConcurrencyPtr; i++ {
		go workerVisit()
	}

	// Add first URL
	utils.CrawlerBar.Add(1)
	UrlChan <- *GoCrawlerOptions.UrlPtr

	// If report option
	//if *GoCrawlerOptions.ReportPtr == true {
	//	gopstaticcrawler.WriteRessourceListReport(append(External_ressources, Internal_ressources...))
	//}

	//if *GoCrawlerOptions.ScreenshotPtr == true {
	//	// Generate template
	//	f, err := os.Create("./index.html")
	//	if err != nil {
	//		utils.Log.Println(err)
	//	}
	//	f.WriteString(screenshot.GetScreenshotHTML(ScreenshotList))
	//}

	// Wait for all workers to finish
	progressBars.Wait()
	defer close(UrlChan)


	// Stop time
	end := time.Now()

	// Print Statistics
	gopstaticcrawler.PrintRessourcesResume("Internal", *GoCrawlerOptions.UrlPtr, &Internal_ressources)
	gopstaticcrawler.PrintRessourcesResume("External", *GoCrawlerOptions.UrlPtr, &External_ressources)

	gopstaticcrawler.PrintStatistics(end.Sub(begin), &Internal_ressources, &External_ressources)
}
